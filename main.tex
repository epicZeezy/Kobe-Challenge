\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{textcomp}
\title{CS63 Spring 2017\\Kobe Shot Selection Project}
\author{Aziz Mansaray and Colin Pillsbury}
\date{}

\begin{document}

\maketitle

\section{Introduction}

We decided to take on the Kobe Shot selection challenge on Kaggle. Kobe Bryant is arguably one of the greatest basketball players of all time. Kobe won 5 NBA championships, 2 NBA finals MVP, 2 NBA scoring championships, and even more accolades. Using 20 years of data on Kobe\textquotesingle s missed and made shots, we decided to test out multiple classifiers to predict the probability that Kobe makes a field goal. 

The following algorithms we tested were: Random Forest, AdaBoost, and Decision Trees. Our reasons for using Random Forest are that it runs efficiently on large data bases, it is effective in finding out which variables are important. Also, by picking the best split for random subsets of the data, it decreases the variance of our results. We used AdaBoost because it\textquotesingle s less likely to overfit than most of the other learning algorithms we could\textquotesingle ve used because it uses multiple weak classifiers. We used Decision Trees because they do not make assumptions about the data and are less prone to being affected heavily by skewed variables. Kobe didnâ€™t play nearly as well the last 3 years of his career as he did the first 17 years so using a classifier that would not be heavily influenced by skewed variables was important to us. 

In addition, we specifically chose to use Random Forest, AdaBoost, and Decision Trees because they require less preprocessing than learning algorithms like K-Nearest. K-Nearest would require us to both quantify and normalize all the relevant categorical features within our dataset. For example, we\textquotesingle d have to quantify ``shot\textunderscore type'' and ''combine\textunderscore shot\textunderscore type'' in a way where both features can be processed, normalized, and contribute enough to the classification of the data point. Given the time constraint that we had for the project, preparing the data set for K-Nearest would not be the most efficient use of our time.

The approach we employed focused on testing different parameters for each algorithm to find out the optimal combination of parameters for producing the best accuracy rates. After analyzing our results and finding out the algorithm which produced the best accuracy rates, we tested the best parameters on each of the algorithms to make sure we were certain this was the algorithm that produced the best results. 


\section{Method and Details}
Our data set, which we got from a Kaggle competition, contains information about every shot that Kobe took during his 20-year-long career.

We started by preprocessing the data. The raw data set contains 25 features and 30697 entries. Initially, we were concerned about the high dimensionality and size of the data set. In general, high dimensionality is unfavorable, so we decided to combine some of the features from the data set. However, we noticed that some of the features were either irrelevant or redundant for our model. For example, we combined the ``minutes\textunderscore remaining'' and ``seconds\textunderscore remaining'' features into a single feature: ``remaining\textunderscore time.'' Similarly, ``game\textunderscore event\textunderscore id,'' which describes when the shot took place during the entire sequence of the game, is better represented by the more quantitative feature ``remaining\textunderscore time,'' so we decided to drop it from the data set.

We dropped certain features that were correlated with other features to help reduce the dimensionality of our data set. The features ``lat'' and ``lon'' (latitude and longitude) are directly correlated with ``loc\textunderscore x'' and ``loc\textunderscore y,'' so we decided to drop ``lat'' and ``lon'' from the data set and to just use ``loc\textunderscore x'' and ``loc\textunderscore y.'' Other features like ``shot\textunderscore zone\textunderscore area,'' ``shot\textunderscore zone\textunderscore range,'' and ``shot\textunderscore zone\textunderscore basic,'' which categorize the shot by its position on the court, were dropped because that information is captured by the ``loc\textunderscore x'' and ``loc\textunderscore y'' features. 

Certain features did not make sense for our given data set. The ``team\textunderscore name'' and ``team\textunderscore id'' features, which tells which team Kobe was playing on, were unnecessary because Kobe played for the Los Angeles Lakers for his entire career. We also decided to replace the ``matchup'' feature with a simpler ``home\textunderscore play'' feature to indicate whether the Lakers were playing at home or away. 

In order to run our models on the data, we also needed to convert all of our categorical variables into separate binary features by using dummy variables. The categorical variables that we had to convert were: ``action\textunderscore type,'' ``combined\textunderscore shot \textunderscore type,'' ``shot\textunderscore type,'' ``opponent,'' ``period,'' and ``season.''

After dropping the unnecessary features, we removed all of the data points that did not contain a value for the ``shot\textunderscore made\textunderscore feature'' (these are the points used for testing in the actual competition). We then divided the data into the explanatory training set and the response training set, which would be used to train our model.

To find the best model, we decided to work with three different classifiers: Random Forest, AdaBoost, and Decision Trees. We began by testing to find the best parameters for each of the models. When testing the performance of each model, we used three-fold cross-validation. We then took the average accuracy of ten rounds of cross-validation and used the accuracy to compare results.

For Random Forest, the parameters we tested are num\textunderscore estimators (the number of trees in the forest), max\textunderscore depth (maximum depth for each tree), and max\textunderscore features (maximum number of features to consider when deciding on best split). Tables 1-3 show the parameter values that we tested and their resulting accuracy scores for Random Forest:

\begin{table}[H]
\centering
\caption{Number of Estimators tested for Random Forest}
\label{my-label}
\begin{tabular}{lllllll}
Number of Estimators & Avg Score &  &  &  &  &  \\
1                    & 0.648870  &  &  &  &  &  \\
10                   & 0.676577  &  &  &  &  &  \\
100                  & 0.681636  &  &  &  &  &  \\
200                  & 0.682064  &  &  &  &  & 
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Max Depths tested for Random Forest}
\label{my-label}
\begin{tabular}{lllllll}
Max Depths & Average Score &  &  &  &  &  \\
1          & 0.612407  &  &  &  &  &  \\
10         & 0.681636  &  &  &  &  &  \\
100        & 0.665331  &  &  &  &  &  \\
200        & 0.665331  &  &  &  &  &  \\
None       & 0.665331  &  &  &  &  & 
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Max Features tested for Random Forest}
\label{my-label}
\begin{tabular}{lllllll}
Max Features & Average Score &  &  &  &  &  \\
25\%         & 0.682337  &  &  &  &  &  \\
50\%         & 0.680508  &  &  &  &  &  \\
75\%         & 0.679730  &  &  &  &  &  \\
100\%        & 0.680196  &  &  &  &  & 
\end{tabular}
\end{table}

Random Forest performed better as the number of estimators increased. However, the improvements plateaued around 200 estimators. The best max depth found was 10. Having too much depth caused the accuracy to drop a bit. The best max feature value was at 25\% of all possible features. Adding more features caused the accuracy to to drop slightly.
The resulting best parameters and the best accuracy for Random Forest are: 


\begin{table}[H]
\centering
\caption{Random Forest Resulting Best Parameters and Accuracy rate}
\label{my-label}
\begin{tabular}{llllll}
Best Max Features & Best Number of Estimators & Best Max Depth & Best Accuracy       &  &  \\
25\%              & 200                       & 10             & 0.68241 &  & 
\end{tabular}
\end{table}


For AdaBoost, the parameters we tested were max\textunderscore depth (maximum depth of the trees) and max\textunderscore features. We decided to use decision trees as the base estimator for AdaBoost because decision trees are a weak classifier that we are familiar with. Tables 5 and 6 show the parameter values that we tested and their resulting accuracy scores:

\begin{table}[H]
\centering
\caption{Number of Estimators tested for AdaBoost}
\label{my-label}
\begin{tabular}{llll}
Number of Estimators & Average Score &  &  \\
10                   & 0.679224      &  &  \\
25                   & 0.681169      &  &  \\
50                   & 0.680547      &  &  \\
100                  & 0.680080      &  & 
\end{tabular}
\end{table}

AdaBoost performed better as the number of estimators increased until the number of estimators was higher than 25. At 25, it seems the accuracy score slowly plateaus once we get to 50 number of estimators and 100 number of estimators. AdaBoost plateaus with a smaller number of estimators (25) than Random Forest does (200). Our intuition is that because Random Forest randomly picks which features to analyze, it takes more estimators to analyze the data. AdaBoost on the other hand, uses weak classifier learners, but each subsequent learner is informed by the group's overall performance. 

\begin{table}[H]
\centering
\caption{Learning Rate tested for AdaBoost}
\label{my-label}
\begin{tabular}{lll}
Learning Rate & Average Score &  \\
0.010000      & 0.659494      &  \\
.10000        & 0.659650      &  \\
.50000        & .679552       &  \\
1.000000      & 0.681169      & 
\end{tabular}
\end{table}


Although AdaBoost made predictions with the highest accuracy when it had a learning rate of 1.00, having a learning rate of 1.00 always runs the risk of possibly ovefitting.

The resulting best parameters and the best accuracy for AdaBoost are:

\begin{table}[H]
\centering
\caption{Best Parameters and resulting accuracy for AdaBoost}
\label{my-label}
\begin{tabular}{lll}
Number of Estimators & Learning Rate & Best Accuracy Score \\
25                   & 1.0           & 0.681170
\end{tabular}
\end{table}

For Decision Trees, the parameters that we tested are max\textunderscore depth (maximum depth of the tree) and max\textunderscore features (the maximum number of features to consider when deciding on the best split).
Tables 8 and 9 list the parameter values that we tested and the resulting accuracy scores for Decision Trees:

\begin{table}[H]
\centering
\caption{Max Features tested for Decision Trees}
\label{my-label}
\begin{tabular}{ll}
Max Features & Average Score \\
.2500        & 0.668405            \\
.5000        & 0.665020            \\
.7500        & 0.664475            \\
1.000        & 0.662802           
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Max Depths tested for Decision Trees}
\label{my-label}
\begin{tabular}{ll}
Max Depth & Average Score \\
1         & 0.659494            \\
5         & 0.678679            \\
10        & 0.662802            \\
25        & 0.600148            \\
100       & 0.583804            \\
200       & 0.583804            \\
None      & 0.583804           
\end{tabular}
\end{table}

For Decision Trees, the best value of the max\textunderscore features parameter was 25\% (same as with with Random Forest). The accuracy decreased as the max\textunderscore feature value approached 100\%. For max\textunderscore depth, the best depth we found was 5. Like with Random Forest, the accuracy decreased as max\textunderscore depth became very large.
The resulting best parameters and the best accuracy for Decision Trees are:

\begin{table}[H]
\centering
\caption{Best Parameters and Resulting Accuracy for Decision Trees}
\label{my-label}
\begin{tabular}{lll}
Best Max Depth & Best Max Feature & Best Accuracy Score \\
5              & .2500            & 0.64105
\end{tabular}
\end{table}


\section{Results}

After finding the best parameters for each of the classifiers, we decided to test the performance of the three models using their best classifiers. Again, we took the average of ten rounds of three-fold cross-validation to evaluate the performance of the classifiers. The table below shows the results from these tests:

\begin{table}[H]
\centering
\caption{Best Accuracy scores for each Classifier}
\label{my-label}
\begin{tabular}{ll}
Classifier      & Best Accuracy Score \\
Random Forest   & 0.68241 \\
AdaBoost        & 0.68117 \\
Decision Treets & 0.64105
\end{tabular}
\end{table}

As the table shows, Random Forest and AdaBoost performed significantly better than their constituent classifier Decision Trees. This is not unusual, because the main purpose of ensemble methods is to improve on single classifiers. The results show that Random Forest performed slightly better than AdaBoost. This difference in accuracy is very small, and could be accounted for by the relatively small number of parameters that we tested for each model. Because we did not test every possible parameter, it is impossible for us to have generated the ideal parameters for each model. As such, we are not able to conclude that Random Forest or AdaBoost is better than the other for our task. However, we decided to use the Random Forest model to explore further. 

Prediction for a basketball shot is inherently difficult. A player could take the identical shot twice, and make it one of the times and miss it the other time. Additionally, there is a lot of information that is not captured in the given data. For example, there is no information about how well-guarded Kobe was when taking the shot. Obviously, a wide open shot would have a higher probability of going in than a heavily-contested double-teamed shot. There are so many factors that affect whether or not a shot goes in that make it very difficult to predict with certainty. 

Ensemble methods like Random Forest often behave like a black box, in that it is difficult to tell how they are learning to predict, and what features they consider to be more important than others. We decided to explore our Random Forest model by measuring its performance on certain subsets of the data. We created three subsets of the data: Layups, Dunks, and Fadeaway Jump Shots. After training the model on the full data set, we ran three-fold cross-validation on each of the three subsets. The table below illustrates the resulting accuracy scores:



\begin{table}[H]
\centering
\caption{Accuracy Rates for Shot Type}
\label{my-label}

\begin{tabular}{lllll}
                         & Fadeaway Jump Shot & Layup   & Dunk    &  \\
\textbf{Number of Shots} & 872                & 4532    & 1056    &  \\
\textbf{Accuracy}        & 0.63073            & 0.71470 & 0.93561 &  \\
                         &                    &         &         & 
\end{tabular}
\end{table}



    The tests on the subsets yielded interesting results. Our classifier performed very well on Dunks, with an accuracy of 93.56\%. This result is not too surprising, as dunks are very high percentage shots. Similarly, layups yielded a relatively high accuracy of 71.47\%. Layups, like dunks, are a high percentage shot, with many players in the league shooting above 60\% on layups. 
    
However, on fadeaway jump shots our classifier had an accuracy of only 63.07\%. Fadeaway jump shots are considered to be one of the most difficult shots in basketball, as it takes a lot of finesse to make a shot while one's momentum is moving away from the hoop. Fadeaway shots are almost always contested too, because the fadeaway is a method of creating space to shoot over a defender. These factors make fadeaway shots highly variable. Because of the high variance and relatively small number of fadeaway shots, it is reasonable that our classifier would underperform on this subset.


\section{Conclusions}

Our goal for this project was to test and compare how various classfication methods perform on a data set that contains all of Kobe Bryant's 30697 shots. After preprocessing the data and testing different classifiers, we found that Random Forest and AdaBoost both performed well, with accuracy scores of over 68\%. Although this accuracy score did not seem impressive at first, after reasoning about the variability and unseen factors that affect a shot we realized that an accuracy of 68\% is fairly impressive.

Further testing showed us that our model performs well on high-percentage, low-variability shots such as dunks and layups. However, our model struggled with high variability shots such as fadeaway jumpers. Ultimately, this project revealed to us the difficulty of a binary classification problem like shot prediction. Even with a relatively large amount of information about a shot, it is still hard to predict the result of a shot with certainty. 

There has been a recent rise in the use of statistics and machine learning in basketball, likely attributed to the success of Sabermetrics in baseball. Currently, one of the hotter topics in the basketball world is the value of ``advanced statistics''. Many teams are utilizing more complex statistics in order to calculate the value of players and playstyles. Earlier this year, the Philadelphia 76ers announced their hiring of an analytics team that consists of a PhD candidate in Computer Science from Harvard, a Statistics professor from UC Berkeley, and a data scientist from IBM. In the same vein, the Toronto Raptors also announced that they have been working with IBM to evaluate draft picks and free agents. As the revenue of the NBA continues to swell, the value of machine learning and data anylsis in basketball will continue to rise.

\end{document}
